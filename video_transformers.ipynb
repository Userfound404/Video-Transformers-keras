{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCjuifwTTxasFdytinP5jS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Userfound404/Video-Transformers-keras/blob/main/video_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9pA2L7DFEUhW"
      },
      "outputs": [],
      "source": [
        "!wget -q https://git.io/JGc31 -O ucf101_top5.tar.gz\n",
        "!tar xf ucf101_top5.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/tensorflow/docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkW0b_XJEeGc",
        "outputId": "b2f3205c-ff66-4e1b-da37-f4f12f95d407"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow_docs.vis import embed\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import imageio\n",
        "import cv2\n",
        "import os"
      ],
      "metadata": {
        "id": "FOS_bDQiEWal"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LENGTH = 20\n",
        "NUM_FEATURES = 1024\n",
        "IMG_SIZE = 128\n",
        "\n",
        "EPOCHS = 5"
      ],
      "metadata": {
        "id": "NaxTAG1VEcSk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script is loading and preparing a dataset for training a video classification model using the Keras library. The script starts by loading two datasets, one for training and one for testing using the pd.read_csv function from the pandas library. The script is using a helper function load_video to load videos from a specified file path, using the OpenCV library. The script is also using a helper function crop_center to crop the center of each frame of the video to a specified image size. The script then creates a feature extractor model using the DenseNet121 architecture and pre-trained on ImageNet dataset, that will be used to extract features from each frame of the video. Then the script applies label preprocessing on the training dataset, using the StringLookup class from Keras.\n",
        "\n",
        "The script then uses the feature extractor and the load_video function to extract features from each frame of all videos in the training dataset and store these features in an array. The script also pads shorter videos to a fixed length (MAX_SEQ_LENGTH), so that all videos have the same length. And It also shows the unique label for the training dataset\n",
        "\n",
        "It is likely that this script is meant to be used as a part of a larger program, and that this script is only preparing the dataset for use in training a video classification model."
      ],
      "metadata": {
        "id": "eGV_D3QIGQLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "print(f\"Total videos for training: {len(train_df)}\")\n",
        "print(f\"Total videos for testing: {len(test_df)}\")\n",
        "\n",
        "center_crop_layer = layers.CenterCrop(IMG_SIZE, IMG_SIZE)\n",
        "\n",
        "\n",
        "def crop_center(frame):\n",
        "    cropped = center_crop_layer(frame[None, ...])\n",
        "    cropped = cropped.numpy().squeeze()\n",
        "    return cropped"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92RAiP_2EyJU",
        "outputId": "5b6923a1-85b0-4bfa-a1bf-562fab69bfef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total videos for training: 594\n",
            "Total videos for testing: 224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_video(path, max_frames=0):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = crop_center(frame)\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)"
      ],
      "metadata": {
        "id": "al7gKiHfHwuj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cap = cv2.VideoCapture(path): This line creates a VideoCapture object from OpenCV library, that reads the video from the specified path.\n",
        "\n",
        "frames = []: This line initializes an empty list to store the frames of the video.\n",
        "\n",
        "while True:: This line starts a while loop that will run until the video has been read completely.\n",
        "\n",
        "ret, frame = cap.read(): This line reads a frame from the video, which is stored in the frame variable. The ret variable stores a Boolean value, indicating whether a frame was successfully read or not.\n",
        "\n",
        "if not ret: break: This line checks the value of the ret variable. If ret is False, which means that no frame was read, the loop will break, and the script will proceed to the next step.\n",
        "\n",
        "frame = crop_center(frame): This line applies the helper function crop_center() to the current frame, which crops the center of the frame to a specified image size.\n",
        "\n",
        "frame = frame[:, :, [2, 1, 0]]: This line reorders the color channels of the frame from BGR to RGB.\n",
        "\n",
        "frames.append(frame): This line adds the current frame to the list of frames.\n",
        "\n",
        "if len(frames) == max_frames: break: This line checks the number of frames that have been read. If the number of frames equals to max_frames, the loop breaks.\n",
        "\n",
        "cap.release(): This line releases the video capturing object.\n",
        "\n",
        "return np.array(frames): This line converts the list of frames to a numpy array and returns it."
      ],
      "metadata": {
        "id": "Wm3ZDIb4HyFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xblBJbT1H4Oz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}